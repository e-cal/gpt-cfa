{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpath = \"../e-cal/gpt-cfa/attempts/\"\n",
    "suffixes = [\"l1\", \"l2\", \"l1/cot\", \"l2/cot\"] #\"l1/cotam\", \"l2/cotam\"]\n",
    "model_names = [\"chatgpt\", \"gpt4\"]\n",
    "all_evals = {}\n",
    "\n",
    "for suffix in suffixes:\n",
    "    all_suffix_results = {\n",
    "        model_name: [] for model_name in model_names\n",
    "    }\n",
    "    suffixpath = os.path.join(bpath, suffix)\n",
    "    for file in os.listdir(suffixpath):\n",
    "        filepath = os.path.join(suffixpath, file)\n",
    "        if not os.path.isdir(filepath) and \".csv\" in file:\n",
    "            for model_name in all_suffix_results:\n",
    "                if model_name in file:\n",
    "                    df = pd.read_csv(filepath)\n",
    "                    df.correct = df.correct.apply(lambda v: 1 if v == \"yes\" else 0)\n",
    "                    all_suffix_results[model_name].append(df)\n",
    "                    break\n",
    "    for model_name in model_names:\n",
    "        try:\n",
    "            all_suffix_results[model_name] = pd.concat(all_suffix_results[model_name], axis=0)\n",
    "        except:\n",
    "            del all_suffix_results[model_name]\n",
    "    all_evals[suffix] = all_suffix_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error analysis:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. On l1 exams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a - Understanding CoT performance discrepancies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt = all_evals[\"l1\"][\"chatgpt\"]\n",
    "chatgpt_cot = all_evals[\"l1/cot\"][\"chatgpt\"]\n",
    "failures_chatgpt = chatgpt[chatgpt.correct < 1]\n",
    "failures_chatgpt_cot = chatgpt_cot[chatgpt_cot.correct < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_by_chatgpt_cot_but_not_zeroshot = failures_chatgpt_cot[~failures_chatgpt_cot.id.isin(failures_chatgpt.id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(failed_by_chatgpt_cot_but_not_zeroshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 28 # 28\n",
    "# r = failed_by_chatgpt_cot_but_not_zeroshot.iloc[k]\n",
    "k = 235\n",
    "r = failed_by_chatgpt_cot_but_not_zeroshot[failed_by_chatgpt_cot_but_not_zeroshot.id == k].iloc[0]\n",
    "print(f\"ID: {r.id}\\tQUESTION: {r.question}\")\n",
    "print(f\"A. {r.choice_a}\\tB. {r.choice_b}\\tC. {r.choice_c}\")\n",
    "print(f\"ANSWER: {r.answer}\\tEXPLANATION: {r.explanation}\")\n",
    "print(\"#############\\n\")\n",
    "print(f\"GUESS: {r.guess}\")\n",
    "print(f\"THINKING: {r.thinking}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly computation questions again\n",
    "\n",
    "- 391: (computations) Was not able to correctly use the evidence? Thus, incorrect reasoning, and also incorrect approx at the end\n",
    "- 406: (knowledge) The steps are correct, but the final answer selected is not... (not enough reasoning)\n",
    "- 407: (computations) The steps are correct, but the computation is wrong (answer is rounded to just 1 significative nb which causes the pb?)\n",
    "- 408: (computations) The steps are incorrect; used too much evidence tho the computations were correct\n",
    "- 409: (computations) Uses the correct formula, but the steps that lead to the values populated in the formula are wrong => not using the evidence super well?\n",
    "- 410: (knowledge) Steps of reasoning are correct, but the model seemingly did not really understand that option A (selected) was negated?\n",
    "- 411: (knowledge) Either ChatGPT hallucinated some facts about agents II in GIPS in its definitions, or the question is very treacherous\n",
    "- 412: (knowledge) One of the step of the reasoning is incorrect (ChatGPT extrapolates a bit too much the provided info)\n",
    "- 413: (knowledge) One of the step of the reasoning is incorrect (ChatGPT extrapolated a bit too much the provided info)\n",
    "- 782: (computations) Wrong formula of covariance \n",
    "- 752: (computations) Steps are correct, formula is correct, result is correct, but picked the wrong option?\n",
    "- 781: (computations) Steps are correct, formula is correct, but some of the calculations are not returning the correct values (see Year 1)\n",
    "- 769: (computations) Steps are correct, formula is correct, result is correct, but picked the wrong option?\n",
    "- 770: (applied knowledge) introduced some knowledge, but DID NOT read correctly option C (which says permanent =/= temporary => ChatGPT did not necessarily use the right reasoning/knowledge to make its guess)\n",
    "- 779: (computations & applied knowledge) considered the right formula, estimated the tax benefit well, but did not conclude correctly?\n",
    "- 735: (knowledge) made the right analysis of each evidenced comment, but did not make the right conclusion based on this analysis (ended up saying both were violations)\n",
    "- 474: (knowledge) reasoning unclear? Might have hallucinated some info about DCF?\n",
    "- 507: (computations) formula incomplete, lacking some computation steps to get final expected result\n",
    "- 455: (computations) considered the right formula, obtained the right results for calculations, but did not conclude correctly?\n",
    "- 478: (computations) correct steps, but incorrect formulas/computations => wrong result\n",
    "- 224: (computations) correct steps, correct formula, correct numbers, but final result of the calculations incorrect\n",
    "- 255: (knowledge) wrong knowledge involved (steps seem legit, but the LLM does not seem to be invocating CFA-specific knowledge => hallucinations)\n",
    "- 264: (computations) used the right formula, did all the right calculations and got the right result, yet selected the wrong answer...\n",
    "- 235: (computations) used the right formula, did all the right calculations and got the right result, yet selected the wrong answer...\n",
    "- 189: (knowledge) did not invocate any CFA-specific knowledge (standards of practice) in order to answer => incorrect\n",
    "- 343: (knowledge) incorrect reasoning (cited the right definition, but did not seem to understand it fully in order to elect the right answer)\n",
    "- 290: (computations) used the right formula, did all the right calculations and got the right result, yet selected the wrong answer...\n",
    "- 296: (knowledge) incorrect reasoning; hallucinated definitions/knowledge instead of invocating the correct CFA knowledge\n",
    "- 69: (computations) correct reasoning, correct calculations and results, but did not read correctly the final ratio/fraction it obtained"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like, with CoT, ChatGPT sometimes got confused with its own explanations/computations and returned the wrong answer (VS zeroshot; issue with zeroshot is that it could very well be simply guessing the right answer => unclear/blackbox)\n",
    "=> we're exposing ourselves more to extrapolations, exagerations, hallucinations, when using CoT on a domain an LLM is not super comfortable in it seems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion: Mostly errors linked to inaccuracies in the computation-related questions. Some cases where it is not relying on the right knowledge to answer. Some other cases where everything is correct almost until the end of the reasoning, but then a mistake is made or the \"answer\" function is not used correctly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_by_chatgpt_zeroshot_but_not_cot = failures_chatgpt[~failures_chatgpt.id.isin(failures_chatgpt_cot.id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(failed_by_chatgpt_zeroshot_but_not_cot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, given the initial strange observation we made when comparing performance, there are a bit less failure cases from zeroshot that were correctly answered by CoT (as opposed to what everyone reports regarding CoT being better than zeroshot). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4 = all_evals[\"l1\"][\"gpt4\"]\n",
    "gpt4_cot = all_evals[\"l1/cot\"][\"gpt4\"]\n",
    "failures_gpt4 = gpt4[gpt4.correct < 1]\n",
    "failures_gpt4_cot = gpt4_cot[gpt4_cot.correct < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_by_gpt4_zeroshot_but_not_cot = failures_gpt4[~failures_gpt4.id.isin(failures_gpt4_cot.id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(failed_by_gpt4_zeroshot_but_not_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_by_gpt4_zeroshot_but_not_cot = failures_gpt4_cot[~failures_gpt4_cot.id.isin(failures_gpt4.id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(failed_by_gpt4_zeroshot_but_not_cot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, once again, the performance difference between CoT and no CoT is very small, so the numbers are not that different, even though we observe that GPT4_CoT > GPT_zeroshot here, as in the performance comparison. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_evals[\"l1\"][\"chatgpt\"].correct.apply(lambda v: 1 if v == \"yes\" else 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_evals[\"l1\"][\"gpt4\"].correct.apply(lambda v: 1 if v == \"yes\" else 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_evals[\"l1\"][\"chatgpt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_evals[\"l1\"][\"gpt4\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfa_exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
